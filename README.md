# ðŸ¤– [Project Name] â€” Autonomous Conversational Robotic Assistant

> **HackPSU 2025 Submission**  
> *Built for the future of embodied intelligence â€” where robots can see, listen, understand, and act.*

![HackPSU Badge](https://img.shields.io/badge/HackPSU-2025-blue?style=for-the-badge)
![Gemini API](https://img.shields.io/badge/Gemini_2.5_Flash-Enabled-orange?style=for-the-badge)
![ElevenLabs](https://img.shields.io/badge/ElevenLabs-Voice_AI-red?style=for-the-badge)
![Raspberry Pi](https://img.shields.io/badge/Raspberry_Pi-3B+-green?style=for-the-badge)
![Arduino](https://img.shields.io/badge/Arduino-UNO_R3-blue?style=for-the-badge)

---

## ðŸŒŸ Overview

**[Project Name]** is an **autonomous, conversational, and vision-enabled robotic assistant** designed to enhance elderly care and hospital support through AI-driven reasoning, speech, and perception.  

By fusing **natural language understanding**, **computer vision**, and **real-world robotics**, our system demonstrates the next step in *embodied intelligence* â€” robots that **see, listen, understand, and act** to serve human needs.

---

## ðŸ§  Core Architecture

### âš™ï¸ Hardware Stack
- **4 High-torque DC motors** enabling two-axis movement and rotation.  
- **Full-HD Camera** for live vision input and contextual object detection.  
- **Microphone + Speaker** for seamless two-way audio communication.  
- **Front-mounted Grabber Claw** for object manipulation and delivery.  
- **Ultrasonic Distance Sensor** for navigation and obstacle avoidance.  
- **Dual Controllers:**  
  - ðŸŸ¦ *Arduino UNO R3* for low-level control and actuation.  
  - ðŸ“ *Raspberry Pi 3B+* for AI inference, data processing, and system coordination.

---

### ðŸ’» Software Stack
- **ðŸ§  Gemini 2.5 Flash API** â€” Multimodal reasoning, NLU, and vision-based task planning.  
- **ðŸ—£ï¸ ElevenLabs API** â€” Realistic, emotionally expressive text-to-speech synthesis.  
- **ðŸŽ¥ OpenCV + Gemini Vision API** â€” Real-time object segmentation, tracking, and scene understanding.  
- **âš¡ Custom Python â†” Arduino Bridge** â€” Real-time synchronization between speech, vision, and motor control.

---

## ðŸ¤– Capabilities

**[Project Name]** can:
- ðŸ’¬ Engage in *real-time conversations* using natural speech.  
- ðŸ‘ï¸ Recognize and interpret *visual context*, e.g., â€œFind the red mug near the window.â€  
- ðŸš— Execute *autonomous locomotion and manipulation* tasks (navigate, pick up, deliver).  
- ðŸ§© Combine *vision + language reasoning* to translate spoken commands into physical actions.  

---

## â¤ï¸ Real-World Applications

In **elderly homes and hospitals**, **[Project Name]** can:
- ðŸ’Š Deliver medication, food, or tools to patients and nurses.  
- ðŸ—£ï¸ Respond to vocal cues for help, reassurance, or conversation.  
- ðŸ¤ Assist caregivers by automating repetitive tasks and promoting patient safety.  

Beyond healthcare, it can also serve in **education, accessibility, and humanitarian robotics**, as a general framework for conversational AI agents with embodiment.

---

## ðŸ† Hackathon Alignment

### ðŸ§  MLH â€” *Best Use of Gemini API*
> Harnesses **Gemini 2.5 Flash** as the reasoning and perception core, enabling natural-language-driven control of autonomous systems and bridging human intent with robotic execution.

### ðŸŽ¤ MLH â€” *Best Use of ElevenLabs API*
> Integrates **ElevenLabs** to provide lifelike, emotionally expressive voice synthesis â€” turning sterile machine speech into warm, human-centered interaction.

### ðŸŒ Nittany AI Challenge â€” *Health & Humanitarian Track*
> Addresses real-world challenges in **elderly care and healthcare environments** by combining safe robotics with conversational AI to empower caregivers and enhance quality of life.

---

## ðŸ§© System Architecture Diagram

```

Human Speech â†’ Gemini Reasoning â†’ Vision & Context Analysis â†’ Motor Commands â†’ Physical Action
â†“
ElevenLabs Voice Output

````

---

## ðŸ‘¥ Team

| Name          | Role                          | Focus                                         |
| --------------| ----------------------------- | --------------------------------------------- |
| Gustavo Foz   | AI & Api Integrations         | Gemini Integration, ElevenLabs Integration    |
| Julien Mutton | Hardware & Robotics           | Arduino Control System,                       |
| Liang Tao Hu  | Software Engineer & Design    | Raspberry Pi Integration, FastAPI Server      |
| Zy Tran       | Electronics & Computer Vision | Electronics, System Design, Segmentation      |

---

## ðŸ’¡ Vision â­

> â€œTo make assistive robotics emotionally intelligent, context-aware, and truly human-centered.â€

---

## ðŸš€ What's Next

### ðŸ§© Short-Term
- Smarter grabber with servo precision and adaptive grip.  
- Basic memory for people, objects, and spaces.  
- SLAM-based mapping and smoother navigation.  
- Web dashboard for live monitoring and control.

### ðŸŒ Mid-Term
- Integration with healthcare and wearable data.  
- Voice-guided accessibility support for mobility or vision needs.  
- Edge optimization for offline AI processing.

### ðŸ’­ Long-Term
> Evolve [Project Name] into a general-purpose assistive AI framework â€” where robots think, act, and genuinely help.

---

### ðŸ’™ Thank you, HackPSU!
> Built with passion, purpose, and a sleepless weekend â€” dedicated to advancing human-centered robotics and AI for Good!

```
