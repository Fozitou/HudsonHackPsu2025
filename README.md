# 🤖 [Project Name] — Autonomous Conversational Robotic Assistant

> **HackPSU 2025 Submission**  
> *Built for the future of embodied intelligence — where robots can see, listen, understand, and act.*

![HackPSU Badge](https://img.shields.io/badge/HackPSU-2025-blue?style=for-the-badge)
![Gemini API](https://img.shields.io/badge/Gemini_2.5_Flash-Enabled-orange?style=for-the-badge)
![ElevenLabs](https://img.shields.io/badge/ElevenLabs-Voice_AI-red?style=for-the-badge)
![Raspberry Pi](https://img.shields.io/badge/Raspberry_Pi-3B+-green?style=for-the-badge)
![Arduino](https://img.shields.io/badge/Arduino-UNO_R3-blue?style=for-the-badge)

---

## 🌟 Overview

**[Project Name]** is an **autonomous, conversational, and vision-enabled robotic assistant** designed to enhance elderly care and hospital support through AI-driven reasoning, speech, and perception.  

By fusing **natural language understanding**, **computer vision**, and **real-world robotics**, our system demonstrates the next step in *embodied intelligence* — robots that **see, listen, understand, and act** to serve human needs.

---

## 🧠 Core Architecture

### ⚙️ Hardware Stack
- **4 High-torque DC motors** enabling two-axis movement and rotation.  
- **Full-HD Camera** for live vision input and contextual object detection.  
- **Microphone + Speaker** for seamless two-way audio communication.  
- **Front-mounted Grabber Claw** for object manipulation and delivery.  
- **Ultrasonic Distance Sensor** for navigation and obstacle avoidance.  
- **Dual Controllers:**  
  - 🟦 *Arduino UNO R3* for low-level control and actuation.  
  - 🍓 *Raspberry Pi 3B+* for AI inference, data processing, and system coordination.

---

### 💻 Software Stack
- **🧠 Gemini 2.5 Flash API** — Multimodal reasoning, NLU, and vision-based task planning.  
- **🗣️ ElevenLabs API** — Realistic, emotionally expressive text-to-speech synthesis.  
- **🎥 OpenCV + Gemini Vision API** — Real-time object segmentation, tracking, and scene understanding.  
- **⚡ Custom Python ↔ Arduino Bridge** — Real-time synchronization between speech, vision, and motor control.

---

## 🤖 Capabilities

**[Project Name]** can:
- 💬 Engage in *real-time conversations* using natural speech.  
- 👁️ Recognize and interpret *visual context*, e.g., “Find the red mug near the window.”  
- 🚗 Execute *autonomous locomotion and manipulation* tasks (navigate, pick up, deliver).  
- 🧩 Combine *vision + language reasoning* to translate spoken commands into physical actions.  

---

## ❤️ Real-World Applications

In **elderly homes and hospitals**, **[Project Name]** can:
- 💊 Deliver medication, food, or tools to patients and nurses.  
- 🗣️ Respond to vocal cues for help, reassurance, or conversation.  
- 🤝 Assist caregivers by automating repetitive tasks and promoting patient safety.  

Beyond healthcare, it can also serve in **education, accessibility, and humanitarian robotics**, as a general framework for conversational AI agents with embodiment.

---

## 🏆 Hackathon Alignment

### 🧠 MLH — *Best Use of Gemini API*
> Harnesses **Gemini 2.5 Flash** as the reasoning and perception core, enabling natural-language-driven control of autonomous systems and bridging human intent with robotic execution.

### 🎤 MLH — *Best Use of ElevenLabs API*
> Integrates **ElevenLabs** to provide lifelike, emotionally expressive voice synthesis — turning sterile machine speech into warm, human-centered interaction.

### 🌍 Nittany AI Challenge — *Health & Humanitarian Track*
> Addresses real-world challenges in **elderly care and healthcare environments** by combining safe robotics with conversational AI to empower caregivers and enhance quality of life.

---

## 🧩 System Architecture Diagram

```

Human Speech → Gemini Reasoning → Vision & Context Analysis → Motor Commands → Physical Action
↓
ElevenLabs Voice Output

````

---

## 👥 Team

| Name          | Role                          | Focus                                         |
| --------------| ----------------------------- | --------------------------------------------- |
| Gustavo Foz   | AI & Api Integrations         | Gemini Integration, ElevenLabs Integration    |
| Julien Mutton | Hardware & Robotics           | Arduino Control System,                       |
| Liang Tao Hu  | Software Engineer & Design    | Raspberry Pi Integration, FastAPI Server      |
| Zy Tran       | Electronics & Computer Vision | Electronics, System Design, Segmentation      |

---

## 💡 Vision ⭐

> “To make assistive robotics emotionally intelligent, context-aware, and truly human-centered.”

---

## 🚀 What's Next

### 🧩 Short-Term
- Smarter grabber with servo precision and adaptive grip.  
- Basic memory for people, objects, and spaces.  
- SLAM-based mapping and smoother navigation.  
- Web dashboard for live monitoring and control.

### 🌐 Mid-Term
- Integration with healthcare and wearable data.  
- Voice-guided accessibility support for mobility or vision needs.  
- Edge optimization for offline AI processing.

### 💭 Long-Term
> Evolve [Project Name] into a general-purpose assistive AI framework — where robots think, act, and genuinely help.

---

### 💙 Thank you, HackPSU!
> Built with passion, purpose, and a sleepless weekend — dedicated to advancing human-centered robotics and AI for Good!

```
